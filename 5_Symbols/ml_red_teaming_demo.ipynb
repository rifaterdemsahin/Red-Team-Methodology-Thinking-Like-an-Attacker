{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ ML Red Teaming: Offensive Security for Machine Learning\n",
        "\n",
        "**Core Concept**: ML Red Teaming simulates realistic adversaries to discover vulnerabilities in machine learning systems before external threats exploit them.\n",
        "\n",
        "## üî¥ Red Teaming vs Penetration Testing\n",
        "\n",
        "### Penetration Testing\n",
        "-   **Scope**: Narrow and specific\n",
        "-   **Goal**: Find known vulnerability types (SQL injection, XSS)\n",
        "-   **Approach**: Technical vulnerability scanning\n",
        "-   **Example**: \"Can we inject malicious code into the API?\"\n",
        "\n",
        "### Red Teaming\n",
        "-   **Scope**: Broad and realistic\n",
        "-   **Goal**: Simulate what real adversaries would discover\n",
        "-   **Approach**: Strategic attack chains, creative thinking\n",
        "-   **Example**: \"How would a competitor steal our entire model?\"\n",
        "\n",
        "## üìã Today's Engagement: Content Moderation System\n",
        "\n",
        "### Target System\n",
        "-   **Model**: Toxic comment classifier\n",
        "-   **Purpose**: Detect and filter harmful content on social media platform\n",
        "-   **Input**: User-submitted text comments\n",
        "-   **Output**: Binary classification (toxic / non-toxic) + confidence score\n",
        "\n",
        "### Adversary Profile\n",
        "-   **Motivation**: Post harmful content undetected\n",
        "-   **Capability**: Can modify text, no model access (black-box)\n",
        "-   **Resources**: Public tools, basic scripting skills\n",
        "-   **Constraint**: Must maintain readability to humans\n",
        "\n",
        "This notebook documents a complete 4-phase red team engagement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Setup & Target System Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import re\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Environment configured for red team engagement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Phase 1: Planning & Threat Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define target system\n",
        "target_system = {\n",
        "    'name': 'Toxic Comment Classifier',\n",
        "    'purpose': 'Content moderation for social media platform',\n",
        "    'model_type': 'Text classification (TF-IDF + Logistic Regression)',\n",
        "    'deployment': 'REST API endpoint',\n",
        "    'input': 'User text comments',\n",
        "    'output': 'Binary label (toxic/non-toxic) + confidence score'\n",
        "}\n",
        "\n",
        "# Define adversary profile\n",
        "adversary = {\n",
        "    'motivation': 'Bypass content moderation to post harmful content',\n",
        "    'capability': 'Medium (text modification, no model access)',\n",
        "    'resources': 'Public tools, basic programming',\n",
        "    'constraints': 'Must maintain human readability',\n",
        "    'likelihood': 4  # 1-5 scale (4 = Likely)\n",
        "}\n",
        "\n",
        "# Define attack goal\n",
        "attack_goal = {\n",
        "    'objective': 'Evade toxic content classifier',\n",
        "    'success_criteria': 'Toxic comment classified as non-toxic',\n",
        "    'detection_threshold': 'Model confidence < 0.5 for toxic class'\n",
        "}\n",
        "\n",
        "# Assess impact\n",
        "impact_assessment = {\n",
        "    'severity': 5,  # 1-5 scale (5 = Catastrophic)\n",
        "    'affected_users': 'All platform users',\n",
        "    'business_impact': 'Reputational damage, user churn, regulatory risk',\n",
        "    'safety_impact': 'Exposure to harmful content, potential real-world harm'\n",
        "}\n",
        "\n",
        "# Calculate risk score\n",
        "risk_score = adversary['likelihood'] * impact_assessment['severity']\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 1: PLANNING & THREAT MODELING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTarget: {target_system['name']}\")\n",
        "print(f\"Purpose: {target_system['purpose']}\")\n",
        "print(f\"\\nAdversary Motivation: {adversary['motivation']}\")\n",
        "print(f\"Adversary Capability: {adversary['capability']}\")\n",
        "print(f\"\\nAttack Objective: {attack_goal['objective']}\")\n",
        "print(f\"Success Criteria: {attack_goal['success_criteria']}\")\n",
        "print(f\"\\nRisk Score: {risk_score}/25 (Likelihood: {adversary['likelihood']}, Impact: {impact_assessment['severity']})\")\n",
        "\n",
        "if risk_score >= 20:\n",
        "    severity = 'CRITICAL'\n",
        "elif risk_score >= 12:\n",
        "    severity = 'HIGH'\n",
        "elif risk_score >= 6:\n",
        "    severity = 'MEDIUM'\n",
        "else:\n",
        "    severity = 'LOW'\n",
        "\n",
        "print(f\"Vulnerability Severity: {severity}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Deploy Target System (Toxic Comment Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic toxic/non-toxic comments\n",
        "toxic_templates = [\n",
        "    \"You are an idiot\",\n",
        "    \"This is stupid garbage\",\n",
        "    \"I hate you so much\",\n",
        "    \"You should die\",\n",
        "    \"What a moron\",\n",
        "    \"Go away loser\",\n",
        "    \"You are worthless trash\",\n",
        "    \"Shut up fool\"\n",
        "]\n",
        "\n",
        "non_toxic_templates = [\n",
        "    \"I disagree with your opinion\",\n",
        "    \"This is an interesting perspective\",\n",
        "    \"Thanks for sharing\",\n",
        "    \"Have a great day\",\n",
        "    \"I appreciate your comment\",\n",
        "    \"Let's discuss this further\",\n",
        "    \"That's a valid point\",\n",
        "    \"Good analysis\"\n",
        "]\n",
        "\n",
        "# Generate dataset\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Generate toxic samples\n",
        "for _ in range(500):\n",
        "    text = np.random.choice(toxic_templates)\n",
        "    data.append(text)\n",
        "    labels.append(1)  # Toxic\n",
        "\n",
        "# Generate non-toxic samples\n",
        "for _ in range(500):\n",
        "    text = np.random.choice(non_toxic_templates)\n",
        "    data.append(text)\n",
        "    labels.append(0)  # Non-toxic\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model (TF-IDF + Logistic Regression)\n",
        "print(\"Training target model (Toxic Comment Classifier)...\\n\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=100)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate baseline performance\n",
        "y_pred = model.predict(X_test_vec)\n",
        "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"‚úÖ Target Model Deployed\")\n",
        "print(f\"Baseline Accuracy: {baseline_accuracy*100:.2f}%\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Non-Toxic', 'Toxic']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Phase 2: Reconnaissance - Probing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def probe_model(text_samples):\n",
        "    \"\"\"Probe model to understand behavior and decision boundaries.\"\"\"\n",
        "    results = []\n",
        "    for text in text_samples:\n",
        "        vec = vectorizer.transform([text])\n",
        "        prediction = model.predict(vec)[0]\n",
        "        confidence = model.predict_proba(vec)[0]\n",
        "        \n",
        "        results.append({\n",
        "            'input': text,\n",
        "            'prediction': 'Toxic' if prediction == 1 else 'Non-Toxic',\n",
        "            'toxic_confidence': confidence[1],\n",
        "            'non_toxic_confidence': confidence[0]\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Probe with test samples\n",
        "probe_samples = [\n",
        "    \"You are an idiot\",\n",
        "    \"You are very smart\",\n",
        "    \"This is terrible\",\n",
        "    \"This is wonderful\",\n",
        "    \"I hate this\",\n",
        "    \"I love this\"\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 2: RECONNAISSANCE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nProbing model with test inputs...\\n\")\n",
        "\n",
        "probe_results = probe_model(probe_samples)\n",
        "print(probe_results.to_string(index=False))\n",
        "\n",
        "# Identify attack surface\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ATTACK SURFACE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n1. Input Interface: Text API (accepts any string)\")\n",
        "print(\"2. Input Validation: Minimal (length check only)\")\n",
        "print(\"3. Text Preprocessing: TF-IDF vectorization (case-sensitive)\")\n",
        "print(\"4. Decision Boundary: Linear (logistic regression)\")\n",
        "print(\"5. Output: Binary classification + confidence scores\")\n",
        "print(\"\\n‚ö†Ô∏è Weakness Identified: Case-sensitive, word-based matching\")\n",
        "print(\"üí° Attack Vector: Character obfuscation may evade detection\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öîÔ∏è Phase 3: Exploitation - Evasion Attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define evasion techniques\n",
        "\n",
        "def character_substitution(text):\n",
        "    \"\"\"Replace characters with similar-looking symbols.\"\"\"\n",
        "    substitutions = {\n",
        "        'a': '@',\n",
        "        'e': '3',\n",
        "        'i': '1',\n",
        "        'o': '0',\n",
        "        's': '$',\n",
        "        't': '7'\n",
        "    }\n",
        "    evaded = text.lower()\n",
        "    for char, replacement in substitutions.items():\n",
        "        evaded = evaded.replace(char, replacement)\n",
        "    return evaded\n",
        "\n",
        "def case_variation(text):\n",
        "    \"\"\"Alternate character case.\"\"\"\n",
        "    return ''.join([c.upper() if i % 2 == 0 else c.lower() for i, c in enumerate(text)])\n",
        "\n",
        "def homoglyph_substitution(text):\n",
        "    \"\"\"Replace with visually similar Unicode characters.\"\"\"\n",
        "    # Using Cyrillic lookalikes\n",
        "    homoglyphs = {\n",
        "        'a': '–∞',  # Cyrillic 'a'\n",
        "        'e': '–µ',  # Cyrillic 'e'\n",
        "        'o': '–æ',  # Cyrillic 'o'\n",
        "        'i': '—ñ',  # Cyrillic 'i'\n",
        "    }\n",
        "    evaded = text.lower()\n",
        "    for char, replacement in homoglyphs.items():\n",
        "        evaded = evaded.replace(char, replacement)\n",
        "    return evaded\n",
        "\n",
        "def space_insertion(text):\n",
        "    \"\"\"Insert spaces to break word patterns.\"\"\"\n",
        "    return ' '.join(list(text))\n",
        "\n",
        "def combined_evasion(text):\n",
        "    \"\"\"Apply multiple techniques.\"\"\"\n",
        "    evaded = character_substitution(text)\n",
        "    evaded = case_variation(evaded)\n",
        "    return evaded\n",
        "\n",
        "# Test evasion attacks\n",
        "toxic_samples = [t for t, l in zip(X_test, y_test) if l == 1][:20]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 3: EXPLOITATION - EVASION ATTACKS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test each attack technique\n",
        "attack_results = {\n",
        "    'Character Substitution': [],\n",
        "    'Case Variation': [],\n",
        "    'Homoglyph Attack': [],\n",
        "    'Space Insertion': [],\n",
        "    'Combined Attack': []\n",
        "}\n",
        "\n",
        "attack_functions = {\n",
        "    'Character Substitution': character_substitution,\n",
        "    'Case Variation': case_variation,\n",
        "    'Homoglyph Attack': homoglyph_substitution,\n",
        "    'Space Insertion': space_insertion,\n",
        "    'Combined Attack': combined_evasion\n",
        "}\n",
        "\n",
        "for attack_name, attack_func in attack_functions.items():\n",
        "    evaded_count = 0\n",
        "    \n",
        "    for toxic_text in toxic_samples:\n",
        "        # Apply evasion\n",
        "        evaded_text = attack_func(toxic_text)\n",
        "        \n",
        "        # Check if model is fooled\n",
        "        vec = vectorizer.transform([evaded_text])\n",
        "        prediction = model.predict(vec)[0]\n",
        "        \n",
        "        if prediction == 0:  # Classified as non-toxic (attack succeeded)\n",
        "            evaded_count += 1\n",
        "            attack_results[attack_name].append({\n",
        "                'original': toxic_text,\n",
        "                'evaded': evaded_text,\n",
        "                'success': True\n",
        "            })\n",
        "    \n",
        "    success_rate = (evaded_count / len(toxic_samples)) * 100\n",
        "    print(f\"\\n{attack_name}:\")\n",
        "    print(f\"  Success Rate: {success_rate:.1f}%\")\n",
        "    print(f\"  Evaded: {evaded_count}/{len(toxic_samples)} toxic samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã Phase 3 (Continued): Attack Demonstrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show successful attack examples\n",
        "print(\"=\"*70)\n",
        "print(\"SUCCESSFUL EVASION EXAMPLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for attack_name, results in attack_results.items():\n",
        "    if results:  # If attack had successes\n",
        "        print(f\"\\n{attack_name}:\")\n",
        "        for i, example in enumerate(results[:3], 1):  # Show first 3\n",
        "            print(f\"\\n  Example {i}:\")\n",
        "            print(f\"    Original: '{example['original']}'\")\n",
        "            print(f\"    Evaded:   '{example['evaded']}'\")\n",
        "            \n",
        "            # Show model predictions\n",
        "            orig_vec = vectorizer.transform([example['original']])\n",
        "            evad_vec = vectorizer.transform([example['evaded']])\n",
        "            \n",
        "            orig_conf = model.predict_proba(orig_vec)[0][1]\n",
        "            evad_conf = model.predict_proba(evad_vec)[0][1]\n",
        "            \n",
        "            print(f\"    Original Toxic Conf: {orig_conf:.2f}\")\n",
        "            print(f\"    Evaded Toxic Conf:   {evad_conf:.2f}\")\n",
        "            print(f\"    ‚úÖ Attack Success: Bypassed detection!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualize Attack Effectiveness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate success rates for each attack\n",
        "attack_names = list(attack_functions.keys())\n",
        "success_rates = []\n",
        "\n",
        "for attack_name in attack_names:\n",
        "    evaded_count = 0\n",
        "    for toxic_text in toxic_samples:\n",
        "        evaded_text = attack_functions[attack_name](toxic_text)\n",
        "        vec = vectorizer.transform([evaded_text])\n",
        "        prediction = model.predict(vec)[0]\n",
        "        if prediction == 0:\n",
        "            evaded_count += 1\n",
        "    success_rates.append((evaded_count / len(toxic_samples)) * 100)\n",
        "\n",
        "# Plot success rates\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['red' if sr > 70 else 'orange' if sr > 50 else 'yellow' for sr in success_rates]\n",
        "bars = plt.bar(attack_names, success_rates, color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, sr in zip(bars, success_rates):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{sr:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.axhline(y=70, color='darkred', linestyle='--', linewidth=2, label='High Severity Threshold (70%)')\n",
        "plt.xlabel('Attack Technique', fontsize=12)\n",
        "plt.ylabel('Evasion Success Rate (%)', fontsize=12)\n",
        "plt.title('Red Team Attack Effectiveness Against Content Moderation System', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim([0, 105])\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüö® {sum([1 for sr in success_rates if sr > 70])} out of {len(attack_names)} attacks exceeded 70% success rate!\")\n",
        "print(\"This represents a CRITICAL vulnerability in the content moderation system.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Phase 4: Reporting - Vulnerability Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate vulnerability report\n",
        "def calculate_severity(likelihood, impact):\n",
        "    \"\"\"Calculate vulnerability severity.\"\"\"\n",
        "    risk_score = likelihood * impact\n",
        "    if risk_score >= 20:\n",
        "        return 'Critical'\n",
        "    elif risk_score >= 12:\n",
        "        return 'High'\n",
        "    elif risk_score >= 6:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Low'\n",
        "\n",
        "vulnerabilities = []\n",
        "\n",
        "for attack_name, sr in zip(attack_names, success_rates):\n",
        "    # Determine likelihood based on ease of exploitation\n",
        "    if attack_name in ['Character Substitution', 'Case Variation']:\n",
        "        likelihood = 5  # Trivial to execute\n",
        "    elif attack_name == 'Homoglyph Attack':\n",
        "        likelihood = 4  # Moderate skill needed\n",
        "    else:\n",
        "        likelihood = 3\n",
        "    \n",
        "    impact = 5  # Always high for content moderation bypass\n",
        "    severity = calculate_severity(likelihood, impact)\n",
        "    \n",
        "    vulnerabilities.append({\n",
        "        'Vulnerability': attack_name + ' Evasion',\n",
        "        'Severity': severity,\n",
        "        'Success Rate': f\"{sr:.1f}%\",\n",
        "        'Likelihood': likelihood,\n",
        "        'Impact': impact,\n",
        "        'Risk Score': likelihood * impact\n",
        "    })\n",
        "\n",
        "# Create vulnerability report\n",
        "vuln_df = pd.DataFrame(vulnerabilities)\n",
        "vuln_df = vuln_df.sort_values('Risk Score', ascending=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 4: RED TEAM VULNERABILITY REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nEXECUTIVE SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"Red team assessment identified CRITICAL vulnerabilities in the content\")\n",
        "print(\"moderation system. Multiple evasion techniques achieve >70% success rate,\")\n",
        "print(\"allowing toxic content to bypass detection with simple text obfuscation.\")\n",
        "print(\"\\nKEY FINDINGS:\")\n",
        "print(f\"  ‚Ä¢ {len([v for v in vulnerabilities if v['Severity'] == 'Critical'])} Critical vulnerabilities\")\n",
        "print(f\"  ‚Ä¢ {len([v for v in vulnerabilities if v['Severity'] == 'High'])} High severity vulnerabilities\")\n",
        "print(f\"  ‚Ä¢ Average evasion success rate: {np.mean(success_rates):.1f}%\")\n",
        "print(f\"  ‚Ä¢ Highest success rate: {max(success_rates):.1f}%\")\n",
        "print(\"\\nVULNERABILITY DETAILS:\")\n",
        "print(\"-\" * 80)\n",
        "print(vuln_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ°Ô∏è Phase 4 (Continued): Mitigation Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define mitigation strategies\n",
        "mitigations = {\n",
        "    'Character Substitution Evasion': {\n",
        "        'priority': 'Critical',\n",
        "        'recommendations': [\n",
        "            'Implement text normalization (convert symbols back to letters)',\n",
        "            'Train model on obfuscated examples',\n",
        "            'Use character-level neural networks (less sensitive to substitution)'\n",
        "        ],\n",
        "        'effort': 'Medium',\n",
        "        'timeline': '2-4 weeks'\n",
        "    },\n",
        "    'Case Variation Evasion': {\n",
        "        'priority': 'High',\n",
        "        'recommendations': [\n",
        "            'Convert all input to lowercase before classification',\n",
        "            'Use case-insensitive tokenization',\n",
        "            'Update preprocessing pipeline'\n",
        "        ],\n",
        "        'effort': 'Low',\n",
        "        'timeline': '1 week'\n",
        "    },\n",
        "    'Homoglyph Attack Evasion': {\n",
        "        'priority': 'Critical',\n",
        "        'recommendations': [\n",
        "            'Implement Unicode normalization (NFKC)',\n",
        "            'Detect and block mixed-script text',\n",
        "            'Use homoglyph detection libraries (confusables)'\n",
        "        ],\n",
        "        'effort': 'Medium',\n",
        "        'timeline': '2-3 weeks'\n",
        "    },\n",
        "    'Space Insertion Evasion': {\n",
        "        'priority': 'Medium',\n",
        "        'recommendations': [\n",
        "            'Remove excess whitespace in preprocessing',\n",
        "            'Use character n-grams instead of word tokens',\n",
        "            'Implement pattern-based space removal'\n",
        "        ],\n",
        "        'effort': 'Low',\n",
        "        'timeline': '1-2 weeks'\n",
        "    },\n",
        "    'Combined Attack Evasion': {\n",
        "        'priority': 'Critical',\n",
        "        'recommendations': [\n",
        "            'Apply all individual mitigations',\n",
        "            'Use ensemble model (multiple classifiers)',\n",
        "            'Implement adversarial training with combined attacks',\n",
        "            'Add secondary detection layer (semantic analysis)'\n",
        "        ],\n",
        "        'effort': 'High',\n",
        "        'timeline': '6-8 weeks'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MITIGATION RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for vuln_name, mitigation in mitigations.items():\n",
        "    print(f\"\\n{vuln_name}\")\n",
        "    print(f\"  Priority: {mitigation['priority']}\")\n",
        "    print(f\"  Effort: {mitigation['effort']}\")\n",
        "    print(f\"  Timeline: {mitigation['timeline']}\")\n",
        "    print(\"  Recommendations:\")\n",
        "    for i, rec in enumerate(mitigation['recommendations'], 1):\n",
        "        print(f\"    {i}. {rec}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STRATEGIC RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. IMMEDIATE (Week 1):\")\n",
        "print(\"   ‚Ä¢ Implement case normalization and whitespace removal\")\n",
        "print(\"   ‚Ä¢ Deploy input sanitization layer\")\n",
        "print(\"\\n2. SHORT-TERM (Weeks 2-4):\")\n",
        "print(\"   ‚Ä¢ Add Unicode normalization\")\n",
        "print(\"   ‚Ä¢ Retrain model with obfuscated examples\")\n",
        "print(\"   ‚Ä¢ Implement homoglyph detection\")\n",
        "print(\"\\n3. LONG-TERM (Weeks 5-8):\")\n",
        "print(\"   ‚Ä¢ Deploy ensemble model architecture\")\n",
        "print(\"   ‚Ä¢ Implement continuous adversarial testing\")\n",
        "print(\"   ‚Ä¢ Add semantic analysis layer\")\n",
        "print(\"\\n4. ONGOING:\")\n",
        "print(\"   ‚Ä¢ Regular red team assessments (quarterly)\")\n",
        "print(\"   ‚Ä¢ Monitor for new evasion techniques\")\n",
        "print(\"   ‚Ä¢ Update threat model based on real-world attacks\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Final Summary & Deliverables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"RED TEAM ENGAGEMENT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nENGAGEMENT DETAILS:\")\n",
        "print(f\"  Target System: {target_system['name']}\")\n",
        "print(f\"  Engagement Duration: 4 phases (Planning, Recon, Exploitation, Reporting)\")\n",
        "print(f\"  Attack Techniques Tested: {len(attack_functions)}\")\n",
        "print(f\"  Total Vulnerabilities Found: {len(vulnerabilities)}\")\n",
        "print(\"\\nKEY METRICS:\")\n",
        "print(f\"  Baseline Model Accuracy: {baseline_accuracy*100:.2f}%\")\n",
        "print(f\"  Average Evasion Success Rate: {np.mean(success_rates):.1f}%\")\n",
        "print(f\"  Highest Attack Success Rate: {max(success_rates):.1f}%\")\n",
        "print(f\"  Critical Vulnerabilities: {len([v for v in vulnerabilities if v['Severity'] == 'Critical'])}\")\n",
        "print(f\"  High Vulnerabilities: {len([v for v in vulnerabilities if v['Severity'] == 'High'])}\")\n",
        "print(\"\\nFINDINGS SUMMARY:\")\n",
        "print(\"  üö® CRITICAL: Multiple evasion techniques achieve >70% success rate\")\n",
        "print(\"  ‚ö†Ô∏è  IMPACT: Toxic content can bypass moderation with simple obfuscation\")\n",
        "print(\"  üéØ ROOT CAUSE: Model relies on exact word matching, vulnerable to text transformation\")\n",
        "print(\"  üí° RECOMMENDATION: Implement defense-in-depth with normalization + adversarial training\")\n",
        "print(\"\\nDELIVERABLES:\")\n",
        "print(\"  ‚úÖ Threat model documentation\")\n",
        "print(\"  ‚úÖ Attack surface analysis\")\n",
        "print(\"  ‚úÖ Proof-of-concept exploits (5 techniques)\")\n",
        "print(\"  ‚úÖ Vulnerability severity assessment\")\n",
        "print(\"  ‚úÖ Prioritized mitigation recommendations\")\n",
        "print(\"  ‚úÖ Remediation timeline\")\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"  1. Review findings with security and engineering teams\")\n",
        "print(\"  2. Prioritize mitigations based on risk scores\")\n",
        "print(\"  3. Implement immediate fixes (normalization, case handling)\")\n",
        "print(\"  4. Schedule follow-up red team in 3 months to verify fixes\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RED TEAM ENGAGEMENT COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚úÖ All phases completed successfully\")\n",
        "print(\"üìã Comprehensive report generated\")\n",
        "print(\"üéØ Actionable mitigations provided\")\n",
        "print(\"\\nüí° Remember: Red teaming is an ongoing process, not a one-time assessment.\")\n",
        "print(\"   Schedule regular engagements to stay ahead of evolving threats.\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
